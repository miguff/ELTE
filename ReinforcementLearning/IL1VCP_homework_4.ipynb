{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "45046d37",
      "metadata": {
        "id": "45046d37"
      },
      "source": [
        "![Logo](https://github.com/Fortuz/rl_education/blob/main/assets/logo.png?raw=1)\n",
        "\n",
        "Made by  **Zolt√°n Barta**\n",
        "\n",
        "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/Fortuz/rl_education/blob/main/9.%20On-policy%20Control/ppo_homework.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bae6bad",
      "metadata": {
        "id": "2bae6bad"
      },
      "source": [
        "# Proximal Policy Optimization (PPO)\n",
        "\n",
        "Proximal Policy Optimization (PPO) is a popular policy gradient algorithm in reinforcement learning. It is especially effective in continuous control tasks. PPO simplifies the trust-region idea from TRPO by using a clipped surrogate objective, allowing for more stable and efficient training without requiring complex optimization techniques.\n",
        "\n",
        "## What is PPO?\n",
        "\n",
        "PPO is an on-policy, actor-critic algorithm with the following features:\n",
        "\n",
        "1. **Actor-Critic Structure**:  \n",
        "   - Actor network: policy $\\pi_\\theta(a|s)$ that selects actions  \n",
        "   - Critic network: value function $V_\\phi(s)$ that evaluates states\n",
        "\n",
        "2. **On-Policy Learning**:  \n",
        "   - Data is collected using the current policy only.\n",
        "\n",
        "3. **Clipped Surrogate Objective**:  \n",
        "   - Avoids large policy updates that destabilize learning.\n",
        "\n",
        "4. **Multiple Epochs per Batch**:  \n",
        "   - Improves sample efficiency by reusing collected data.\n",
        "\n",
        "## PPO Objective Function\n",
        "\n",
        "### Clipped Surrogate Objective\n",
        "\n",
        "Given:\n",
        "\n",
        "- $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}$\n",
        "- $\\hat{A}_t$: estimated advantage\n",
        "\n",
        "The clipped objective is:\n",
        "\n",
        "$$\n",
        "L^{CLIP}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t,\\ \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon)\\hat{A}_t \\right) \\right]\n",
        "$$\n",
        "\n",
        "This prevents the policy from changing too much in a single update step.\n",
        "\n",
        "### Value Function Loss\n",
        "\n",
        "The critic is trained to minimize the squared error:\n",
        "\n",
        "$$\n",
        "L^{VF}(\\phi) = \\mathbb{E}_t \\left[ \\left( V_\\phi(s_t) - V_t^{\\text{target}} \\right)^2 \\right]\n",
        "$$\n",
        "\n",
        "### Entropy Bonus\n",
        "\n",
        "Encourages exploration:\n",
        "\n",
        "$$\n",
        "S[\\pi_\\theta](s_t) = \\mathbb{E}_{a \\sim \\pi_\\theta} [-\\log \\pi_\\theta(a|s_t)]\n",
        "$$\n",
        "\n",
        "### Combined PPO Loss\n",
        "\n",
        "The full loss combines the components:\n",
        "\n",
        "$$\n",
        "L^{PPO} = \\mathbb{E}_t \\left[ L^{CLIP}(\\theta) - c_1 L^{VF}(\\phi) + c_2 S[\\pi_\\theta](s_t) \\right]\n",
        "$$\n",
        "\n",
        "## PPO Algorithm Steps\n",
        "\n",
        "1. Initialize policy and value networks and hyperparameters\n",
        "2. Collect trajectories using the current policy\n",
        "3. Estimate advantages (typically using GAE)\n",
        "4. For several epochs:\n",
        "   - Divide data into mini-batches\n",
        "   - Compute clipped loss, value loss, entropy bonus\n",
        "   - Update network parameters using gradient descent\n",
        "5. Repeat\n",
        "\n",
        "## Generalized Advantage Estimation (GAE)\n",
        "\n",
        "GAE reduces variance in advantage estimation:\n",
        "\n",
        "$$\n",
        "\\hat{A}^{GAE}_t = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}, \\quad \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\n",
        "$$\n",
        "\n",
        "\n",
        "## Why PPO?\n",
        "\n",
        "- Simpler than TRPO to implement\n",
        "- More stable than vanilla policy gradients\n",
        "- Supports multiple training epochs per batch\n",
        "- Scales well to large models and complex tasks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a2f41f64",
      "metadata": {
        "id": "a2f41f64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import random\n",
        "# Import PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e14e61f7",
      "metadata": {
        "id": "e14e61f7"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make('Pendulum-v1')\n",
        "obs,info = env.reset()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce2b6452",
      "metadata": {
        "id": "ce2b6452"
      },
      "source": [
        "### Task: Implement a Policy Network for PPO (Continuous Action Space)\n",
        "\n",
        "In this task, you will implement a multi-layer perceptron (MLP) policy network for a PPO agent operating in a continuous action space.\n",
        "\n",
        "The goal is for the network to output the parameters of a Normal (Gaussian) distribution from which actions can be sampled.\n",
        "\n",
        "---\n",
        "\n",
        "### Requirements:\n",
        "\n",
        "- Subclass `nn.Module`\n",
        "- Use two hidden layers with ReLU activations\n",
        "- The final layer should output the **mean** of the action distribution\n",
        "- Define a learnable parameter `log_std` to represent the log standard deviation\n",
        "- In the `forward()` method, return a `torch.distributions.Normal(mean, std)` object\n",
        "\n",
        "---\n",
        "\n",
        "### Network Structure:\n",
        "\n",
        "- Input: `n_observations` (number of input features)\n",
        "- Hidden Layer 1: 128 units + ReLU\n",
        "- Hidden Layer 2: 128 units + ReLU\n",
        "- Output Layer: `n_actions` units (mean of the Gaussian)\n",
        "- Standard deviation: computed using `torch.exp(log_std)`, where `log_std` is a learnable `nn.Parameter`\n",
        "\n",
        "---\n",
        "\n",
        "### Implementation Notes:\n",
        "\n",
        "- Use `torch.nn.Parameter` for `log_std` so that it can be learned during training\n",
        "- In the `forward()` method, make sure the input is a float tensor on the correct device\n",
        "- If the input is a 1D tensor (single observation), add a batch dimension with `unsqueeze(0)`\n",
        "- Return a `Normal(mean, std)` distribution from `torch.distributions`\n",
        "\n",
        "---\n",
        "\n",
        "This network will allow you to sample continuous actions for a PPO agent and compute log-probabilities needed for training. Make sure to test the output distribution to verify it behaves as expected.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "463121bf",
      "metadata": {
        "id": "463121bf"
      },
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    \"\"\" MLP Actor network for PPO with continuous action space \"\"\"\n",
        "    def __init__(self, n_observations: int, n_actions: int):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        ###################CODE HERE###################\n",
        "        self.Network = nn.Sequential(\n",
        "            nn.Linear(n_observations, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128,128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, n_actions),\n",
        "        )\n",
        "\n",
        "        self.log_std = nn.Parameter(torch.zeros(n_actions))\n",
        "        ################################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Forward pass, returns a Normal (Gaussian) distribution over actions.\n",
        "        \"\"\"\n",
        "        ###################CODE HERE###################\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        x = x.float().to(self.log_std.device)\n",
        "\n",
        "        mean = self.Network(x)\n",
        "        std = torch.exp(self.log_std) \n",
        "\n",
        "        return torch.distributions.Normal(mean, std)\n",
        "\n",
        "\n",
        "        ################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11e3de96",
      "metadata": {
        "id": "11e3de96"
      },
      "source": [
        "### Task: Implement a Value Network for PPO (Critic)\n",
        "\n",
        "In this task, you will build a multi-layer perceptron (MLP) value network that serves as the **critic** in a PPO setup. The value network estimates the **state value** for a given observation, which is used in advantage estimation and value loss computation.\n",
        "\n",
        "---\n",
        "\n",
        "### Requirements:\n",
        "\n",
        "- Subclass `nn.Module`\n",
        "- Use two hidden layers with ReLU activations\n",
        "- The final layer should output a single scalar value per input state\n",
        "- In the `forward()` method, return the estimated value as a tensor\n",
        "\n",
        "---\n",
        "\n",
        "### Network Structure:\n",
        "\n",
        "- Input: `n_observations` (state dimension)\n",
        "- Hidden Layer 1: 128 units + ReLU\n",
        "- Hidden Layer 2: 128 units + ReLU\n",
        "- Output Layer: 1 unit (scalar value)\n",
        "\n",
        "---\n",
        "\n",
        "### Implementation Notes:\n",
        "\n",
        "- Convert input to `torch.float32` if needed\n",
        "- If input is 1D (a single state), add a batch dimension with `unsqueeze(0)`\n",
        "- Use `F.relu()` as the activation function after each hidden layer\n",
        "- The final layer should not apply any activation\n",
        "\n",
        "---\n",
        "\n",
        "This network is used to approximate the expected return (value) of a given state. It will be trained by minimizing the squared difference between predicted values and target returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3cc42c03",
      "metadata": {
        "id": "3cc42c03"
      },
      "outputs": [],
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "    \"\"\" MLP Critic network for PPO \"\"\"\n",
        "    def __init__(self, n_observations: int):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        ###################CODE HERE###################\n",
        "        \n",
        "        self.lin1 = nn.Linear(n_observations, 128)\n",
        "        self.lin2 = nn.Linear(128, 128)\n",
        "        self.out = nn.Linear(128, 1)\n",
        "\n",
        "        ################################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass, returns the estimated state value.\n",
        "        \"\"\"\n",
        "        ###################CODE HERE###################\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        x = x.float().to(self.lin1.weight.device)\n",
        "\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.relu(self.lin2(x))\n",
        "        estimated_value = self.out(x)\n",
        "\n",
        "        return estimated_value\n",
        "        ################################################\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "315aaccb",
      "metadata": {
        "id": "315aaccb"
      },
      "source": [
        "### Task: Implement Generalized Advantage Estimation (GAE)\n",
        "\n",
        "In this task, you will implement the **Generalized Advantage Estimation (GAE)** function. GAE is used in PPO to compute a low-variance and smoother estimate of the advantage function, which guides policy updates.\n",
        "\n",
        "---\n",
        "\n",
        "### Inputs:\n",
        "\n",
        "- `rewards`: Tensor of rewards collected from the environment.\n",
        "- `values`: Estimated state values from the value network at each timestep.\n",
        "- `next_values`: Value predictions for the next states.\n",
        "- `dones`: Tensor indicating episode terminations (1 if done, 0 otherwise).\n",
        "- `gamma`: Discount factor (typically around 0.99).\n",
        "- `lambda_gae`: GAE lambda parameter (typically around 0.95).\n",
        "- `standardize` (optional): Whether to normalize the advantages.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Concepts:\n",
        "\n",
        "1. **TD Residual** (`delta`):  \n",
        "   $$\n",
        "   \\delta_t = r_t + \\gamma \\cdot V_{t+1} \\cdot (1 - \\text{done}_t) - V_t\n",
        "   $$\n",
        "\n",
        "2. **Recursive Advantage Calculation**:  \n",
        "   Starting from the end of the trajectory and moving backward:\n",
        "   $$\n",
        "   A_t = \\delta_t + \\gamma \\lambda \\cdot A_{t+1} \\cdot (1 - \\text{done}_t)\n",
        "   $$\n",
        "\n",
        "3. **Standardization** (optional):  \n",
        "   Normalize advantages to have zero mean and unit variance to improve training stability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3291b5b5",
      "metadata": {
        "id": "3291b5b5"
      },
      "outputs": [],
      "source": [
        "def compute_gae(rewards: torch.Tensor,\n",
        "                values: torch.Tensor,\n",
        "                next_values: torch.Tensor,\n",
        "                dones: torch.Tensor,\n",
        "                gamma: float,\n",
        "                lambda_gae: float,\n",
        "                standardize: bool = True) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes Generalized Advantage Estimation (GAE).\n",
        "    \"\"\"\n",
        "    ###################CODE HERE###################\n",
        "\n",
        "    rewards = rewards.T\n",
        "   \n",
        "\n",
        "    deltas = rewards + gamma * next_values * (1 - dones) - values\n",
        "\n",
        "    advantages = torch.zeros_like(rewards)\n",
        "    gae = 0.0  # scalar, not a tensor\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        gae = deltas[t] + gamma * lambda_gae * (1 - dones[t]) * gae\n",
        "        advantages[t] = gae  # scalar value assigned to scalar slot\n",
        "\n",
        "    if standardize:\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std(unbiased=False) + 1e-8)\n",
        "\n",
        "\n",
        "\n",
        "    ################################################\n",
        "    return advantages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c7769fb4",
      "metadata": {
        "id": "c7769fb4"
      },
      "outputs": [],
      "source": [
        "def collect_data(env, policy_net, max_steps):\n",
        "    observations, actions, rewards, log_probs, dones = [], [], [], [], []\n",
        "\n",
        "    obs, _ = env.reset()\n",
        "    for _ in range(max_steps):\n",
        "        obs_tensor = torch.tensor(obs.flatten(), dtype=torch.float32, device=device)\n",
        "\n",
        "        dist = policy_net(obs_tensor)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action).sum(-1)\n",
        "\n",
        "        next_obs, reward, terminated, truncated, _ = env.step(action.cpu().numpy())\n",
        "        done = terminated or truncated\n",
        "\n",
        "        observations.append(obs_tensor.squeeze(0))  # to keep consistent shape [n]\n",
        "        actions.append(action)\n",
        "        rewards.append(torch.tensor(reward, dtype=torch.float32, device=device))\n",
        "        log_probs.append(log_prob.detach())\n",
        "        dones.append(torch.tensor(done, dtype=torch.float32, device=device))\n",
        "\n",
        "        obs = next_obs\n",
        "        if done:\n",
        "            obs, _ = env.reset()\n",
        "\n",
        "    observations = torch.stack(observations)\n",
        "    actions = torch.stack(actions)\n",
        "    rewards = torch.stack(rewards)\n",
        "    log_probs = torch.stack(log_probs)\n",
        "    dones = torch.stack(dones)\n",
        "\n",
        "    return observations, actions, log_probs, rewards, dones"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24ccdd56",
      "metadata": {
        "id": "24ccdd56"
      },
      "source": [
        "### Task: Implement the training loop\n",
        "Implement the main training loop for the Proximal Policy Optimization (PPO) algorithm. This loop should train a policy network and a value network using data collected from interaction with an environment.\n",
        "\n",
        "---\n",
        "### What the Function Should Do\n",
        "\n",
        "1. **Collect Experience**  \n",
        "   Interact with the environment for a fixed number of steps. Record:\n",
        "   - Observations\n",
        "   - Actions\n",
        "   - Rewards\n",
        "   - Log-probabilities of actions (under the current policy)\n",
        "   - Done flags\n",
        "\n",
        "2. **Estimate Values and Compute Advantages**  \n",
        "   Use the value network to estimate state values. Then, compute advantages using Generalized Advantage Estimation (GAE). Optionally standardize the advantages for numerical stability.\n",
        "\n",
        "3. **Compute Returns**  \n",
        "   Add the computed advantages to the estimated values to get the target returns for value function learning.\n",
        "\n",
        "4. **Optimize Policy and Value Networks**  \n",
        "   For a number of epochs:\n",
        "   - Shuffle and divide the data into mini-batches.\n",
        "   - For each batch:\n",
        "     - Compute the policy loss using the clipped surrogate PPO objective.\n",
        "     - Compute the value loss using mean squared error between predicted and target returns.\n",
        "     - Backpropagate and update both networks using their respective optimizers.\n",
        "\n",
        "5. **Log Progress**  \n",
        "   After each iteration, print or store:\n",
        "   - Total reward collected\n",
        "   - Average policy loss\n",
        "   - Average value loss\n",
        "   - Mean and standard deviation of advantages\n",
        "\n",
        "---\n",
        "\n",
        "### Notes\n",
        "\n",
        "- Detach tensors where appropriate to avoid reusing computation graphs.\n",
        "- Make sure log-probabilities from the old policy are detached before being used in the surrogate loss.\n",
        "- The value targets (`returns`) should not have gradients.\n",
        "\n",
        "This loop should repeat for a specified number of iterations to progressively improve the policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d1056e80",
      "metadata": {
        "id": "d1056e80"
      },
      "outputs": [],
      "source": [
        "def ppo_training_loop(env, policy_net, value_net, optimizer_policy, optimizer_value,\n",
        "                      iterations, max_steps_per_iter, gamma=0.99, lambda_gae=0.95,\n",
        "                      epsilon_clip=0.2, epochs=10, batch_size=64):\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        # --- Collect data ---\n",
        "        observations, actions, log_probs_old, rewards, dones = collect_data(env, policy_net, max_steps_per_iter)\n",
        "        ###################CODE HERE###################\n",
        "\n",
        "        with torch.no_grad():\n",
        "            values = value_net(observations).squeeze(-1)\n",
        "            # To get next values, shift values and add value for the last next state\n",
        "            next_values = torch.zeros_like(values)\n",
        "            next_values[:-1] = values[1:]\n",
        "            next_values[dones == 1] = 0.0  # Terminal next value at end of batch\n",
        "\n",
        "        advantages = compute_gae(\n",
        "            rewards=rewards,\n",
        "            values=values,\n",
        "            next_values=next_values,\n",
        "            dones=dones,\n",
        "            gamma=gamma,\n",
        "            lambda_gae=lambda_gae,\n",
        "            standardize=True\n",
        "        )\n",
        "\n",
        "        returns = advantages + values\n",
        "        advantages = advantages.view(-1)\n",
        "        returns = returns.view(-1)\n",
        "\n",
        "\n",
        "        dataset_size = len(observations)\n",
        "\n",
        "        ################################################\n",
        "        avg_policy_loss = 0.0\n",
        "        avg_value_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for _ in range(epochs):\n",
        "            indices = torch.randperm(dataset_size)\n",
        "\n",
        "            for start in range(0, dataset_size, batch_size):\n",
        "                end = start + batch_size\n",
        "                batch_indices = indices[start:end]\n",
        "                ###################CODE HERE###################\n",
        "                batch_obs = observations[batch_indices]\n",
        "                batch_actions = actions[batch_indices]\n",
        "                batch_old_log_probs = log_probs_old[batch_indices]\n",
        "                batch_advantages = advantages[batch_indices]\n",
        "                batch_returns = returns[batch_indices]\n",
        "\n",
        "                dist = policy_net(batch_obs)\n",
        "                new_log_probs = dist.log_prob(batch_actions).sum(-1)\n",
        "\n",
        "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
        "                clipped_ratio = torch.clamp(ratio, 1 - epsilon_clip, 1 + epsilon_clip)\n",
        "                surrogate_loss = -torch.min(ratio * batch_advantages, clipped_ratio * batch_advantages)\n",
        "                policy_loss = surrogate_loss.mean()\n",
        "\n",
        "                predicted_values = value_net(batch_obs).squeeze(-1)\n",
        "                value_loss = nn.functional.mse_loss(predicted_values, batch_returns)\n",
        "\n",
        "                optimizer_policy.zero_grad()\n",
        "                policy_loss.backward()\n",
        "                optimizer_policy.step()\n",
        "\n",
        "                optimizer_value.zero_grad()\n",
        "                value_loss.backward()\n",
        "                optimizer_value.step()\n",
        "\n",
        "\n",
        "                ################################################\n",
        "\n",
        "                avg_policy_loss += policy_loss.item()\n",
        "                avg_value_loss += value_loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "        avg_policy_loss /= num_batches\n",
        "        avg_value_loss /= num_batches\n",
        "        total_reward = rewards.sum().item()\n",
        "\n",
        "        print(f\"Iteration {iteration + 1}/{iterations} | \"\n",
        "              f\"Total Reward: {total_reward:.2f} | \"\n",
        "              f\"Avg Policy Loss: {avg_policy_loss:.4f} | \"\n",
        "              f\"Avg Value Loss: {avg_value_loss:.4f} | \"\n",
        "              f\"Advantage Mean: {advantages.mean().item():.4f} | Std: {advantages.std().item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4cef1042",
      "metadata": {
        "id": "4cef1042"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "n_observations = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.shape[0]\n",
        "\n",
        "policy_net = PolicyNetwork(n_observations, n_actions).to(device)\n",
        "value_net = ValueNetwork(n_observations).to(device)\n",
        "\n",
        "optimizer_policy = Adam(policy_net.parameters(), lr=3e-4)\n",
        "optimizer_value = Adam(value_net.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cacf4e93",
      "metadata": {
        "id": "cacf4e93"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\local_user\\anaconda3\\envs\\ELTE\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "c:\\Users\\local_user\\anaconda3\\envs\\ELTE\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:245: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'numpy.ndarray'>\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1/50 | Total Reward: -13437.27 | Avg Policy Loss: 0.0315 | Avg Value Loss: 0.0471 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 2/50 | Total Reward: -14754.83 | Avg Policy Loss: -0.0005 | Avg Value Loss: 0.0323 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 3/50 | Total Reward: -12534.59 | Avg Policy Loss: 0.0005 | Avg Value Loss: 0.0273 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 4/50 | Total Reward: -13133.92 | Avg Policy Loss: 0.0004 | Avg Value Loss: 0.0346 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 5/50 | Total Reward: -12626.91 | Avg Policy Loss: 0.0007 | Avg Value Loss: 0.0450 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 6/50 | Total Reward: -12103.74 | Avg Policy Loss: -0.0000 | Avg Value Loss: 0.0511 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 7/50 | Total Reward: -13575.35 | Avg Policy Loss: 0.0002 | Avg Value Loss: 0.0397 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 8/50 | Total Reward: -12749.73 | Avg Policy Loss: 0.0004 | Avg Value Loss: 0.0558 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 9/50 | Total Reward: -13081.16 | Avg Policy Loss: 0.0001 | Avg Value Loss: 0.0690 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 10/50 | Total Reward: -11615.19 | Avg Policy Loss: 0.0002 | Avg Value Loss: 0.0653 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 11/50 | Total Reward: -14021.35 | Avg Policy Loss: -0.0006 | Avg Value Loss: 0.0810 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 12/50 | Total Reward: -13136.90 | Avg Policy Loss: 0.0004 | Avg Value Loss: 0.1007 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 13/50 | Total Reward: -12252.40 | Avg Policy Loss: 0.0003 | Avg Value Loss: 0.1377 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 14/50 | Total Reward: -13594.36 | Avg Policy Loss: -0.0002 | Avg Value Loss: 0.1459 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 15/50 | Total Reward: -12619.50 | Avg Policy Loss: -0.0001 | Avg Value Loss: 0.1384 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 16/50 | Total Reward: -13869.01 | Avg Policy Loss: -0.0005 | Avg Value Loss: 0.1736 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 17/50 | Total Reward: -14001.60 | Avg Policy Loss: 0.0001 | Avg Value Loss: 0.2590 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 18/50 | Total Reward: -13037.83 | Avg Policy Loss: 0.0002 | Avg Value Loss: 0.2700 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 19/50 | Total Reward: -12627.52 | Avg Policy Loss: -0.0003 | Avg Value Loss: 0.2099 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 20/50 | Total Reward: -12775.01 | Avg Policy Loss: 0.0002 | Avg Value Loss: 0.2574 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 21/50 | Total Reward: -13870.60 | Avg Policy Loss: 0.0002 | Avg Value Loss: 0.3253 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 22/50 | Total Reward: -14360.34 | Avg Policy Loss: -0.0001 | Avg Value Loss: 0.3059 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 23/50 | Total Reward: -12850.46 | Avg Policy Loss: 0.0003 | Avg Value Loss: 0.2529 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 24/50 | Total Reward: -13468.72 | Avg Policy Loss: -0.0000 | Avg Value Loss: 0.2891 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 25/50 | Total Reward: -13957.15 | Avg Policy Loss: 0.0002 | Avg Value Loss: 0.4265 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 26/50 | Total Reward: -13010.79 | Avg Policy Loss: -0.0006 | Avg Value Loss: 0.4351 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 27/50 | Total Reward: -12453.38 | Avg Policy Loss: 0.0004 | Avg Value Loss: 0.2933 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 28/50 | Total Reward: -12687.59 | Avg Policy Loss: 0.0001 | Avg Value Loss: 0.4336 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 29/50 | Total Reward: -12547.19 | Avg Policy Loss: -0.0013 | Avg Value Loss: 0.3571 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 30/50 | Total Reward: -12914.71 | Avg Policy Loss: 0.0003 | Avg Value Loss: 0.4328 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 31/50 | Total Reward: -13202.11 | Avg Policy Loss: 0.0002 | Avg Value Loss: 0.4240 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 32/50 | Total Reward: -12971.24 | Avg Policy Loss: 0.0002 | Avg Value Loss: 0.5101 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 33/50 | Total Reward: -12547.06 | Avg Policy Loss: 0.0000 | Avg Value Loss: 0.4285 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 34/50 | Total Reward: -12847.44 | Avg Policy Loss: 0.0001 | Avg Value Loss: 0.5853 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 35/50 | Total Reward: -11745.38 | Avg Policy Loss: 0.0001 | Avg Value Loss: 0.5172 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 36/50 | Total Reward: -13106.03 | Avg Policy Loss: -0.0003 | Avg Value Loss: 0.4225 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 37/50 | Total Reward: -12341.47 | Avg Policy Loss: -0.0004 | Avg Value Loss: 0.6680 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 38/50 | Total Reward: -13045.94 | Avg Policy Loss: -0.0000 | Avg Value Loss: 0.6269 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 39/50 | Total Reward: -12396.76 | Avg Policy Loss: -0.0006 | Avg Value Loss: 0.4301 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 40/50 | Total Reward: -11978.50 | Avg Policy Loss: 0.0002 | Avg Value Loss: 0.4727 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 41/50 | Total Reward: -13030.00 | Avg Policy Loss: -0.0000 | Avg Value Loss: 0.7353 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 42/50 | Total Reward: -12689.53 | Avg Policy Loss: 0.0000 | Avg Value Loss: 0.6884 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 43/50 | Total Reward: -13272.30 | Avg Policy Loss: 0.0001 | Avg Value Loss: 0.6661 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 44/50 | Total Reward: -12763.22 | Avg Policy Loss: -0.0001 | Avg Value Loss: 0.7007 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 45/50 | Total Reward: -12240.67 | Avg Policy Loss: 0.0001 | Avg Value Loss: 0.6925 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 46/50 | Total Reward: -11941.55 | Avg Policy Loss: 0.0000 | Avg Value Loss: 0.6215 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 47/50 | Total Reward: -12013.99 | Avg Policy Loss: 0.0002 | Avg Value Loss: 0.8372 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 48/50 | Total Reward: -10898.67 | Avg Policy Loss: -0.0002 | Avg Value Loss: 0.7434 | Advantage Mean: 0.0000 | Std: 1.0002\n",
            "Iteration 49/50 | Total Reward: -14030.77 | Avg Policy Loss: 0.0002 | Avg Value Loss: 0.6780 | Advantage Mean: -0.0000 | Std: 1.0002\n",
            "Iteration 50/50 | Total Reward: -13565.97 | Avg Policy Loss: -0.0001 | Avg Value Loss: 0.7471 | Advantage Mean: 0.0000 | Std: 1.0002\n"
          ]
        }
      ],
      "source": [
        "ppo_training_loop(\n",
        "    env=env,\n",
        "    policy_net=policy_net,\n",
        "    value_net=value_net,\n",
        "    optimizer_policy=optimizer_policy,\n",
        "    optimizer_value=optimizer_value,\n",
        "    iterations=50,\n",
        "    max_steps_per_iter=2048,\n",
        "    gamma=0.99,\n",
        "    lambda_gae=0.95,\n",
        "    epsilon_clip=0.2,\n",
        "    epochs=10,\n",
        "    batch_size=64\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ELTE",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
